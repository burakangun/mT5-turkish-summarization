{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Burak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.17.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re\n",
    "from zemberek import TurkishMorphology\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/train.csv', on_bad_lines='skip', engine='python')\n",
    "df = df.drop(['id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review # 1\n",
      "Original Text : \n",
      "By . Associated Press . PUBLISHED: . 14:11 EST, 25 October 2013 . | . UPDATED: . 15:36 EST, 25 October 2013 . The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A . State Immunization Program Manager Molly Howell says the risk is low, but officials feel it's important to alert people to the possible exposure. The diocese announced on Monday that Bishop John Folda is taking time off after being diagnosed with hepatitis A. The diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in Italy last month. Symptoms of hepatitis A include fever, tiredness, loss of appetite, nausea and abdominal discomfort. Fargo Catholic Diocese in North Dakota (pictured) is where the bishop is located .\n",
      "\n",
      "\n",
      "Summary Text : \n",
      "Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\n",
      "He contracted the infection through contaminated food in Italy .\n",
      "Church members in Fargo, Grand Forks and Jamestown could have been exposed .\n",
      "===========================================================================================================\n",
      "\n",
      "\n",
      "Review # 2\n",
      "Original Text : \n",
      "(CNN) -- Ralph Mata was an internal affairs lieutenant for the Miami-Dade Police Department, working in the division that investigates allegations of wrongdoing by cops. Outside the office, authorities allege that the 45-year-old longtime officer worked with a drug trafficking organization to help plan a murder plot and get guns. A criminal complaint unsealed in U.S. District Court in New Jersey Tuesday accuses Mata, also known as \"The Milk Man,\" of using his role as a police officer to help the drug trafficking organization in exchange for money and gifts, including a Rolex watch. In one instance, the complaint alleges, Mata arranged to pay two assassins to kill rival drug dealers. The killers would pose as cops, pulling over their targets before shooting them, according to the complaint. \"Ultimately, the (organization) decided not to move forward with the murder plot, but Mata still received a payment for setting up the meetings,\" federal prosecutors said in a statement. The complaint also alleges that Mata used his police badge to purchase weapons for drug traffickers. Mata, according to the complaint, then used contacts at the airport to transport the weapons in his carry-on luggage on trips from Miami to the Dominican Republic. Court documents released by investigators do not specify the name of the drug trafficking organization with which Mata allegedly conspired but says the organization has been importing narcotics from places such as Ecuador and the Dominican Republic by hiding them \"inside shipping containers containing pallets of produce, including bananas.\" The organization \"has been distributing narcotics in New Jersey and elsewhere,\" the complaint says. Authorities arrested Mata on Tuesday in Miami Gardens, Florida. It was not immediately clear whether Mata has an attorney, and police officials could not be immediately reached for comment. Mata has worked for the Miami-Dade Police Department since 1992, including directing investigations in Miami Gardens and working as a lieutenant in the K-9 unit at Miami International Airport, according to the complaint. Since March 2010, he had been working in the internal affairs division. Mata faces charges of aiding and abetting a conspiracy to distribute cocaine, conspiring to distribute cocaine and engaging in monetary transactions in property derived from specified unlawful activity. He is scheduled to appear in federal court in Florida on Wednesday. If convicted, Mata could face life in prison. CNN's Suzanne Presto contributed to this report.\n",
      "\n",
      "\n",
      "Summary Text : \n",
      "Criminal complaint: Cop used his role to help cocaine traffickers .\n",
      "Ralph Mata, an internal affairs lieutenant, allegedly helped group get guns .\n",
      "He also arranged to pay two assassins in a murder plot, a complaint alleges .\n",
      "===========================================================================================================\n",
      "\n",
      "\n",
      "Review # 3\n",
      "Original Text : \n",
      "A drunk driver who killed a young woman in a head-on crash while checking his mobile phone has been jailed for six years. Craig Eccleston-Todd, 27, was driving home from a night at a pub when he received a text message. As he was reading or replying to it, he veered across the road while driving round a bend and smashed into Rachel Titley’s car coming the other way. Craig Eccleston-Todd, 27 (left) was using his mobile phone when he crashed head-on into the car being driven by Rachel Titley, 28 (right). She died later from her injuries . The head-on crash took place in October 2013. Mr Eccleston-Todd's car was barely recognisable (pictured) Police said Eccleston-Todd had drunk at least three or four pints of beer before getting behind the wheel. He was found guilty of causing death by dangerous driving at Portsmouth Crown Court yesterday. Miss Titley, a 28-year-old solicitor’s clerk from Cowes, Isle of Wight, had also spent the evening with friends at a pub but had not drunk any alcohol, police said. She was driving responsibly and there was ‘nothing she could have done to avoid the collision’, they added. Lindsay Pennell, prosecuting, said: ‘Craig Eccleston-Todd’s driving resulted in the tragic death of a young woman, Rachel Titley, a death that could have been avoided. ‘Mr Eccleston-Todd took the decision to pick up his mobile phone whilst driving and, either reading or replying to this text message, was so distracted that he failed to negotiate a left-hand bend, crossing the central white line into the path of Miss Titley’s oncoming car. Miss Titley was pulled the wreckage of her Daihatsu Cuore but died later from her injuries in hospital . ‘Miss Titley [had] a bright future ahead of her. She was also returning home having spent an enjoyable evening with friends and was driving responsibly. ‘She had arranged to contact her friends when she got home to confirm that she had arrived safely. Her friends sadly never heard from her after they parted company. ‘Miss Titley’s death in these circumstances reiterates the danger of using a hand-held mobile phone whilst driving.’ Police were unable to take breath or blood tests from Eccleston-Todd immediately, but in tests several hours after the accident he was only marginally under the drink-drive limit. The judge agreed with police that he would have been over the limit at the time his red Citroen hit Miss Titley’s blue Daihatsu Cuore on a road near Yarmouth, Isle of Wight, on October 11, 2013. His phone records showed he was also texting around the time of the crash. PC Mark Furse, from Hampshire constabulary’s serious collision investigation unit, said: 'Our thoughts are with Rachel's family at this time. She had been out with friends at a pub in Shalfleet that evening, but had not had any alcohol. 'Our investigation showed that there was nothing she could have done to avoid the collision and sadly it cost her her life. 'Mr Eccleston-Todd had left work in Yarmouth and met with friends at a pub where he drank at least three to four pints of lager. He hadn't long left the pub to return home when the collision occurred at around 9.30pm. 'We weren't able to take breath or blood tests from him immediately and although blood taken several hours after the collision showed he was marginally under the limit, we maintain he would have been over the limit at the time of the collision and in summing up today, the judge agreed. 'The analysis of his phone records showed that he was texting on his phone around the time of the collision so it's highly likely this would also have contributed to his dangerous driving and loss of control.' Eccleston-Todd was found guilty of causing death by dangerous driving following a trial at Portsmouth Crown Court (pictured) He added: 'Mr Eccleston-Todd will now spend six years behind bars, but Rachel's family have lost her forever. 'I hope this will make people think twice before drinking any alcohol and getting behind the wheel, or using a phone once they're on the road. 'The dangers of drink driving and driving whilst using a mobile phone are obvious. Those who continue to do so risk spending a substantial time in prison. This case highlights just how tragic the consequences of committing these offences can be.' ‘Mr Eccleston-Todd will now spend six years behind bars, but Rachel’s family have lost her for ever. I hope this will make people think twice before drinking any alcohol and getting behind the wheel, or using a phone once they’re on the road. This case highlights just how tragic the consequences of committing these offences can be.’ Eccleston-Todd, of Newport, Isle of Wight, was also disqualified from driving for eight years after which he will have to complete an extended re-test.\n",
      "\n",
      "\n",
      "Summary Text : \n",
      "Craig Eccleston-Todd, 27, had drunk at least three pints before driving car .\n",
      "Was using phone when he veered across road in Yarmouth, Isle of Wight .\n",
      "Crashed head-on into 28-year-old Rachel Titley's car, who died in hospital .\n",
      "Police say he would have been over legal drink-drive limit at time of crash .\n",
      "He was found guilty at Portsmouth Crown Court of causing death by dangerous driving .\n",
      "===========================================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3): #Looking at the first 3 samples\n",
    "    print(\"Review #\",i+1)\n",
    "    print('Original Text : ')   \n",
    "    print(df.article[i])\n",
    "   \n",
    "    print('\\n\\nSummary Text : ')\n",
    "    print(df.highlights[i])\n",
    "    \n",
    "    print('===========================================================================================================\\n\\n')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"predefined_stopwords = ['acaba', 'ama', 'aslında', 'az', 'bazı', 'belki', 'biri', 'birkaç', 'birşey', 'biz', 'bu',\\n                                                    'çok', 'çünkü', 'da', 'daha', 'de', 'defa', 'diye', 'eğer', 'en', 'gibi', 'hem', 'hep',\\n                                                    'hepsi', 'her', 'hiç', 'için', 'ile', 'ise', 'kez', 'ki', 'kim', 'mı', 'mu', 'mü', 'nasıl',\\n                                                    'ne', 'neden', 'nerde', 'nerede', 'nereye', 'niçin', 'niye', 'o', 'sanki', 'şey', 'siz',\\n                                                    'şu', 'tüm', 've', 'veya', 'ya', 'yani']\\n                                                    \""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''predefined_stopwords = ['acaba', 'ama', 'aslında', 'az', 'bazı', 'belki', 'biri', 'birkaç', 'birşey', 'biz', 'bu',\n",
    "                                                    'çok', 'çünkü', 'da', 'daha', 'de', 'defa', 'diye', 'eğer', 'en', 'gibi', 'hem', 'hep',\n",
    "                                                    'hepsi', 'her', 'hiç', 'için', 'ile', 'ise', 'kez', 'ki', 'kim', 'mı', 'mu', 'mü', 'nasıl',\n",
    "                                                    'ne', 'neden', 'nerde', 'nerede', 'nereye', 'niçin', 'niye', 'o', 'sanki', 'şey', 'siz',\n",
    "                                                    'şu', 'tüm', 've', 'veya', 'ya', 'yani']\n",
    "                                                    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    #Removing unwanted stopwords and characters to create fewer nulls word embeddings\n",
    "\n",
    "    #Convert words to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\\\', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]','', text) \n",
    "\n",
    "    text = text.split()\n",
    "    stops = set(stopwords.words('turkish'))\n",
    "    text = [word for word in text if not word in stops]\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries are complete!\n",
      "Articles are complete!\n"
     ]
    }
   ],
   "source": [
    "clean_summary = []\n",
    "for s in df['highlights']:\n",
    "    clean_summary.append(clean_text(s))\n",
    "\n",
    "print(\"Summaries are complete!\")\n",
    "\n",
    "clean_article = []\n",
    "for a in df['article']:\n",
    "    clean_article.append(clean_text(a))\n",
    "\n",
    "print('Articles are complete!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Review # 1\n",
      "bishop john folda of north dakota is taking time off after being diagnosed he contracted the infection through contaminated food in italy church members in fargo grand forks and jamestown could have been exposed\n",
      "by associated press published 1411 est 25 october 2013 updated 1536 est 25 october 2013 the bishop of the fargo catholic diocese in north dakota has exposed potentially hundreds of church members in fargo grand forks and jamestown to the hepatitis a virus in late september and early october the state health department has issued an advisory of exposure for anyone who attended five churches and took communion bishop john folda pictured of the fargo catholic diocese in north dakota has exposed potentially hundreds of church members in fargo grand forks and jamestown to the hepatitis a state immunization program manager molly howell says the risk is low but officials feel it s important to alert people to the possible exposure the diocese announced on monday that bishop john folda is taking time off after being diagnosed with hepatitis a the diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in italy last month symptoms of hepatitis a include fever tiredness loss of appetite nausea and abdominal discomfort fargo catholic diocese in north dakota pictured is where the bishop is located\n",
      "\n",
      "Clean Review # 2\n",
      "criminal complaint cop used his role to help cocaine traffickers ralph mata an internal affairs lieutenant allegedly helped group get guns he also arranged to pay two assassins in a murder plot a complaint alleges\n",
      "cnn ralph mata was an internal affairs lieutenant for the miamidade police department working in the division that investigates allegations of wrongdoing by cops outside the office authorities allege that the 45yearold longtime officer worked with a drug trafficking organization to help plan a murder plot and get guns a criminal complaint unsealed in us district court in new jersey tuesday accuses mata also known as the milk man of using his role as a police officer to help the drug trafficking organization in exchange for money and gifts including a rolex watch in one instance the complaint alleges mata arranged to pay two assassins to kill rival drug dealers the killers would pose as cops pulling over their targets before shooting them according to the complaint ultimately the organization decided not to move forward with the murder plot but mata still received a payment for setting up the meetings federal prosecutors said in a statement the complaint also alleges that mata used his police badge to purchase weapons for drug traffickers mata according to the complaint then used contacts at the airport to transport the weapons in his carryon luggage on trips from miami to the dominican republic court documents released by investigators do not specify the name of the drug trafficking organization with which mata allegedly conspired but says the organization has been importing narcotics from places such as ecuador and the dominican republic by hiding them inside shipping containers containing pallets of produce including bananas the organization has been distributing narcotics in new jersey and elsewhere the complaint says authorities arrested mata on tuesday in miami gardens florida it was not immediately clear whether mata has an attorney and police officials could not be immediately reached for comment mata has worked for the miamidade police department since 1992 including directing investigations in miami gardens and working as a lieutenant in the k9 unit at miami international airport according to the complaint since march 2010 he had been working in the internal affairs division mata faces charges of aiding and abetting a conspiracy to distribute cocaine conspiring to distribute cocaine and engaging in monetary transactions in property derived from specified unlawful activity he is scheduled to appear in federal court in florida on wednesday if convicted mata could face life in prison cnn s suzanne presto contributed to this report\n",
      "\n",
      "Clean Review # 3\n",
      "craig ecclestontodd 27 had drunk at least three pints before driving car was using phone when he veered across road in yarmouth isle of wight crashed headon into 28yearold rachel titley s car who died in hospital police say he would have been over legal drinkdrive limit at time of crash he was found guilty at portsmouth crown court of causing death by dangerous driving\n",
      "a drunk driver who killed a young woman in a headon crash while checking his mobile phone has been jailed for six years craig ecclestontodd 27 was driving home from a night at a pub when he received a text message as he was reading or replying to it he veered across the road while driving round a bend and smashed into rachel titleys car coming the other way craig ecclestontodd 27 left was using his mobile phone when he crashed headon into the car being driven by rachel titley 28 right she died later from injuries the headon crash took place in october 2013 mr ecclestontodd s car was barely recognisable pictured police said ecclestontodd had drunk at least three or four pints of beer before getting behind the wheel he was found guilty of causing death by dangerous driving at portsmouth crown court yesterday miss titley a 28yearold solicitors clerk from cowes isle of wight had also spent the evening with friends at a pub but had not drunk any alcohol police said she was driving responsibly and there was nothing she could have done to avoid the collision they added lindsay pennell prosecuting said craig ecclestontodds driving resulted in the tragic death of a young woman rachel titley a death that could have been avoided mr ecclestontodd took the decision to pick up his mobile phone whilst driving and either reading or replying to this text message was so distracted that he failed to negotiate a lefthand bend crossing the central white line into the path of miss titleys oncoming car miss titley was pulled the wreckage of daihatsu cuore but died later from injuries in hospital miss titley had a bright future ahead of she was also returning home having spent an enjoyable evening with friends and was driving responsibly she had arranged to contact friends when she got home to confirm that she had arrived safely friends sadly never heard from after they parted company miss titleys death in these circumstances reiterates the danger of using a handheld mobile phone whilst driving police were unable to take breath or blood tests from ecclestontodd immediately but in tests several hours after the accident he was only marginally under the drinkdrive limit the judge agreed with police that he would have been over the limit at the time his red citroen hit miss titleys blue daihatsu cuore on a road near yarmouth isle of wight on october 11 2013 his phone records showed he was also texting around the time of the crash pc mark furse from hampshire constabularys serious collision investigation unit said our thoughts are with rachel s family at this time she had been out with friends at a pub in shalfleet that evening but had not had any alcohol our investigation showed that there was nothing she could have done to avoid the collision and sadly it cost life mr ecclestontodd had left work in yarmouth and met with friends at a pub where he drank at least three to four pints of lager he hadn t long left the pub to return home when the collision occurred at around 930pm we weren t able to take breath or blood tests from him immediately and although blood taken several hours after the collision showed he was marginally under the limit we maintain he would have been over the limit at the time of the collision and in summing up today the judge agreed the analysis of his phone records showed that he was texting on his phone around the time of the collision so it s highly likely this would also have contributed to his dangerous driving and loss of control ecclestontodd was found guilty of causing death by dangerous driving following a trial at portsmouth crown court pictured he added mr ecclestontodd will now spend six years behind bars but rachel s family have lost forever i hope this will make people think twice before drinking any alcohol and getting behind the wheel or using a phone once they re on the road the dangers of drink driving and driving whilst using a mobile phone are obvious those who continue to do so risk spending a substantial time in prison this case highlights just how tragic the consequences of committing these offences can be mr ecclestontodd will now spend six years behind bars but rachels family have lost for ever i hope this will make people think twice before drinking any alcohol and getting behind the wheel or using a phone once theyre on the road this case highlights just how tragic the consequences of committing these offences can be ecclestontodd of newport isle of wight was also disqualified from driving for eight years after which he will have to complete an extended retest\n",
      "\n",
      "Clean Review # 4\n",
      "nina dos santos says europe must be ready to accept sanctions will hurt both sides targeting russia s business community would be one way of sapping their support for president putin she says but she says europe would have a hard time keeping its factories going without power from the east\n",
      "cnn with a breezy sweep of his pen president vladimir putin wrote a new chapter into crimea s turbulent history committing the region to a future returned to russian domain sixty years prior ukraine s breakaway peninsula was signed away just as swiftly by soviet leader nikita khrushchev but dealing with such a blatant land grab on its eastern flank won t be anywhere near as quick and easy for europe s 28member union because unlike crimea s rushed referendum everyone has a say after initially slapping visa restrictions and asset freezes on a limited number of little known politicians and military men europe is facing urgent calls to widen the scope of its measures to target the russian business community in particular the logic of this is that those who run russia and own it are essentially two sides of the coin alexei navalny onetime moscow mayoral contender now under house arrest for opposing the current regime called for europe s leaders to ban everyone from vladimir putin s personal banker to chelsea football club owner roman abramovich from keeping their money and loved ones abroad asset freezes and visa restrictions are especially palatable options for the eu because they can be rolled out on a discretionary basis without requiring cumbersome legal procedures and recourse in fact russia cancels visas for people it doesn t like all the time just look at hermitage capital founder bill browder who lost both his right of entry and moscowbased money in 2005 and dare not go back russia also banned the adoption of its orphans by americans in retaliation for the us s implementation of an anticorruption law named after sergei magnitsky browder s lawyer who died after a year in a moscow detention center apparently beaten to death yet in playing the money talks card europe must be ready for the consequences of such action because money also walks as such eu leaders must be ready to accept sanctions are a twoway street and will hurt both sides targeting russia s peripatetic business community would be one way of sapping their tenuous support for president putin and such a strategy might also turn out to have a silver lining awarding eu countries a chance to finally deal with some of the more unpleasant sides of their patronage including money laundering and corruption which have inflated prize assets like london property and picasso paintings for years where europe should hold fire though is trade two decades of postsoviet rapprochement and almost 500 billion worth of commerce is a lot to put at stake it s true that any trade war would hurt russia far harder than it would the eu not least because 15 of the former s gdp comes from exports to the bloc but europe with its hefty reliance on russian gas would have a hard time keeping its factories going and citizens warm without power from the east and while putin flexes his political muscle open trade channels keep the dialogue going giving all sides a chance to change the subject and talk less tensely no one can afford to cut off that lifeline especially now with europe s economy on the rebound and russia s one on the wane\n",
      "\n",
      "Clean Review # 5\n",
      "fleetwood top of league one after 20 win at scunthorpe peterborough bristol city chesterfield and crawley all drop first points of the season standin striker matt done scores a hattrick as rochdale thrash crewe 52 wins for notts county and yeovil coventrybradford and oldhamport vale both end in draws a late stephen bywater own goal denies gillingham three points against millwall\n",
      "fleetwood are the only team still to have a 100 record in sky bet league one as a 20 win over scunthorpe sent graham alexanders men top of the table the cod army are playing in the third tier for the first time in their history after six promotions in nine years and their remarkable ascent shows no sign of slowing with jamie proctor and gareth evans scoring the goals at glanford park fleetwood were one of five teams to have won two out of two but the other four clubs peterborough bristol city chesterfield and crawley all hit their first stumbling blocks posh were defeated 21 by sheffield united who had lost both of their opening contests jose baxters opener gave the blades a firsthalf lead and although it was later cancelled out by shaun brisleys goal ben davies snatched a winner six minutes from time in the lead jose baxter right celebrates opening the scoring for sheffield united up for the battle sheffield united s michael doyle left challenges peterborough s kyle vassell in a keenlycontested clash bristol city who beat nigel cloughs men on the opening day were held to a goalless draw by last season s playoff finalists leyton orient while chesterfield the league two champions were beaten 10 by mk dons who play manchester united in the capital one cup in seven days time arsenal loanee benik afobe scored the only goal of the game just after the break meanwhile crawley lost their unbeaten status while bradford maintained theirs thanks to a 31 win for the bantams james hanson became the first player to score against crawley this season after 49 minutes before joe walsh equalised five minutes later heads up bristol city s korey smith left and leyton orient s lloyd james go up for a header but strikes from billy knott and mason bennett sealed an impressive away win phil parkinson s men bradford are now second behind fleetwood after doncasters stoppagetime equaliser meant preston for whom joe garner signed a new contract earlier on tuesday were held to a 11 draw which slipped them down the table chris humphrey looked to have secured the points for the lilywhites but nathan tyson struck a lastgasp leveller standin striker matt done scored a hattrick for rochdale in the evenings highscoring affair as crewe were hammered 52 marcus haber marked his full railwaymen debut with a brace but dones treble and goals from ian henderson and peter vincenti helped keith hills men to a big away victory there were plenty of goals between coventry and barnsley too in a 22 draw with all four goals coming in the first half josh mcquoid and jordan clarke twice gave the sky blues the lead but the tykes earned a point thanks to strikes from conor hourihane and leroy lita notts county recorded a 21 home win over colchester with ronan murray and liam noble on target freddie sears replied for colchester james wilson s second half equaliser earned oldham a points against port vale after tom pope s opener and yeovil claimed a 21 away victory at walsall with kevin dawson striking a late winner tom bradshaw had equalised after veteran james hayter gave the glovers the lead finally swindon held gillingham to a 22 draw thanks to stephen bywaters lastminute own goal danny kedwell and kortney hause twice gave the gills the lead but andy williams pulled swindon level before bywater dropped raphael branco s cross into his own net\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"Clean Review #\",i+1)\n",
    "    print(clean_summary[i])\n",
    "    print(clean_article[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(count_dict, text):\n",
    "    '''Count the number of occurrences of each word in a set of text'''\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 730965\n"
     ]
    }
   ],
   "source": [
    "# Find the number of times each word was used and the size of the vocabulary\n",
    "word_counts = {}\n",
    "count_words(word_counts, clean_summary)\n",
    "count_words(word_counts, clean_article)          \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 516783\n"
     ]
    }
   ],
   "source": [
    "# Load Conceptnet Numberbatch's (CN) embeddings, similar to GloVe, but probably better \n",
    "# (https://github.com/commonsense/conceptnet-numberbatch)\n",
    "embeddings_index = {}\n",
    "with open('../numberbatch-en.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from CN: 33927\n",
      "Percent of words that are missing from vocabulary: 4.64%\n"
     ]
    }
   ],
   "source": [
    "# Find the number of words that are missing from CN, and are used more than our threshold.\n",
    "missing_words = 0\n",
    "threshold = 20\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
    "            \n",
    "print(\"Number of words missing from CN:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 730965\n",
      "Number of words we will use: 168639\n",
      "Percent of words we will use: 23.07%\n"
     ]
    }
   ],
   "source": [
    "# Limit the vocab that we will use to words that appear ≥ threshold or are in GloVe\n",
    "\n",
    "#dictionary to convert words to integers\n",
    "vocab_to_int = {} \n",
    "\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        vocab_to_int[word] = value\n",
    "        value += 1\n",
    "\n",
    "# Special tokens that will be added to our vocab\n",
    "codes = [\"\",\"\",\"\",\"\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168640\n"
     ]
    }
   ],
   "source": [
    "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "embedding_dim = 300\n",
    "nb_words = max(vocab_to_int.values()) + 1 \n",
    "\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in CN, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
    "    '''Convert words in text to an integer.\n",
    "       If word is not in vocab_to_int, use UNK's integer.\n",
    "       Total the number of words and UNKs.\n",
    "       Add EOS token to the end of texts'''\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"\"])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentence_ints.append(vocab_to_int[\"\"])\n",
    "        ints.append(sentence_ints)\n",
    "    return ints, word_count, unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 209646129\n",
      "Total number of UNKs in headlines: 1569488\n",
      "Percent of words that are UNK: 0.75%\n"
     ]
    }
   ],
   "source": [
    "# Apply convert_to_ints to clean_summaries and clean_texts\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "int_summaries, word_count, unk_count = convert_to_ints(clean_summary, word_count, unk_count)\n",
    "int_texts, word_count, unk_count = convert_to_ints(clean_article, word_count, unk_count, eos=True)\n",
    "\n",
    "unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lengths(text):\n",
    "    '''Create a data frame of the sentence lengths from a text'''\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries:\n",
      "              counts\n",
      "count  287113.000000\n",
      "mean       48.288120\n",
      "std        20.298531\n",
      "min         3.000000\n",
      "25%        35.000000\n",
      "50%        45.000000\n",
      "75%        56.000000\n",
      "max      1226.000000\n",
      "\n",
      "Texts:\n",
      "              counts\n",
      "count  287113.000000\n",
      "mean      682.898702\n",
      "std       331.170675\n",
      "min         9.000000\n",
      "25%       437.000000\n",
      "50%       623.000000\n",
      "75%       866.000000\n",
      "max      2171.000000\n"
     ]
    }
   ],
   "source": [
    "lengths_summaries = create_lengths(int_summaries)\n",
    "lengths_texts = create_lengths(int_texts)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(lengths_summaries.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1150.0\n",
      "1344.0\n",
      "1647.0\n"
     ]
    }
   ],
   "source": [
    "print(np.percentile(lengths_texts.counts, 90))\n",
    "print(np.percentile(lengths_texts.counts, 95))\n",
    "print(np.percentile(lengths_texts.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.0\n",
      "85.0\n",
      "113.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of summaries\n",
    "print(np.percentile(lengths_summaries.counts, 90))\n",
    "print(np.percentile(lengths_summaries.counts, 95))\n",
    "print(np.percentile(lengths_summaries.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_counter(sentence):\n",
    "    '''Counts the number of time UNK appears in a sentence.'''\n",
    "    count_unk = 0\n",
    "    for i in sentence:\n",
    "      if i == vocab_to_int['']:\n",
    "        count_unk += 1\n",
    "    return count_unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170520\n",
      "170520\n"
     ]
    }
   ],
   "source": [
    "sorted_summaries = []\n",
    "sorted_texts = []\n",
    "max_text_length = 959\n",
    "max_summary_length = 116\n",
    "min_length = 8\n",
    "unk_text_limit = 1\n",
    "unk_summary_limit = 0\n",
    "\n",
    "for length in range(min(lengths_texts.counts), max_text_length): \n",
    "    for count, words in enumerate(int_summaries):\n",
    "        if (len(int_summaries[count]) >= min_length and\n",
    "            len(int_summaries[count]) <= max_summary_length and\n",
    "            len(int_texts[count]) >= min_length and\n",
    "            unk_counter(int_summaries[count]) <= unk_summary_limit and\n",
    "        \n",
    "            length == len(int_texts[count])\n",
    "           ):\n",
    "            sorted_summaries.append(int_summaries[count])\n",
    "            sorted_texts.append(int_texts[count])\n",
    "        \n",
    "# Compare lengths to ensure they match\n",
    "print(len(sorted_summaries))\n",
    "print(len(sorted_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    '''Create palceholders for inputs to the model'''\n",
    "    \n",
    "    input_data = tf.compat.v1.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.compat.v1.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.compat.v1.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.compat.v1.placeholder(tf.float32, name='keep_prob')\n",
    "    summary_length = tf.compat.v1.placeholder(tf.int32, (None,), name='summary_length')\n",
    "    max_summary_length = tf.compat.v1.reduce_max(summary_length, name='max_dec_len')\n",
    "    text_length = tf.compat.v1.placeholder(tf.int32, (None,), name='text_length')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the  to the begining of each batch'''\n",
    "    \n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
    "    '''Create the encoding layer'''\n",
    "    \n",
    "    enc_cells = []\n",
    "\n",
    "    for layer in range(num_layers):\n",
    "        with tf.compat.v1.variable_scope('encoder_{}'.format(layer)):\n",
    "            cell_fw = tf.keras.layers.LSTM(rnn_size,\n",
    "                                           return_sequences=True,\n",
    "                                           return_state=True,\n",
    "                                           dropout=1 - keep_prob)\n",
    "            cell_bw = tf.keras.layers.LSTM(rnn_size,\n",
    "                                           return_sequences=True,\n",
    "                                           return_state=True,\n",
    "                                           dropout=1 - keep_prob)\n",
    "\n",
    "            enc_cells.append((cell_fw, cell_bw))\n",
    "\n",
    "    enc_outputs = []\n",
    "    enc_states = []\n",
    "    for cell_fw, cell_bw in enc_cells:\n",
    "        output, state_fw, state_bw = tf.keras.layers.Bidirectional(cell_fw)(rnn_inputs)\n",
    "        enc_outputs.append(output)\n",
    "        enc_states.append((state_fw, state_bw))\n",
    "\n",
    "    enc_output = tf.concat(enc_outputs, axis=-1)\n",
    "    enc_state = tf.concat(enc_states, axis=0)\n",
    "\n",
    "    return enc_output, enc_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
    "                            vocab_size, max_summary_length):\n",
    "    '''Create the training logits'''\n",
    "    \n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                        sequence_length=summary_length,\n",
    "                                                        time_major=False)\n",
    "\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                       training_helper,\n",
    "                                                       initial_state,\n",
    "                                                       output_layer) \n",
    "\n",
    "    training_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                           output_time_major=False,\n",
    "                                                           impute_finished=True,\n",
    "                                                           maximum_iterations=max_summary_length)\n",
    "    return training_decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
    "                             max_summary_length, batch_size):\n",
    "    '''Create the inference logits'''\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                end_token)\n",
    "                \n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        initial_state,\n",
    "                                                        output_layer)\n",
    "                \n",
    "    inference_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            output_time_major=False,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return inference_decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
    "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
    "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.compat.v1.variable_scope('decoder_{}'.format(layer)):\n",
    "            lstm = tf.keras.layers.LSTM(rnn_size,\n",
    "                                        return_sequences=True,\n",
    "                                        return_state=True,\n",
    "                                        dropout=1 - keep_prob)\n",
    "            dec_cell = tf.keras.layers.Dropout(lstm, input_keep_prob=keep_prob)\n",
    "    \n",
    "    output_layer = Dense(vocab_size,\n",
    "                         kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "    \n",
    "    attn_mech = tf.keras.layers.Attention()  # BahdanauAttention\n",
    "\n",
    "    dec_cell = tf.keras.layers.AttentionWrapper(dec_cell, attn_mech, rnn_size)\n",
    "            \n",
    "    initial_state = dec_cell.zero_state(batch_size=batch_size, dtype=tf.float32).clone(cell_state=enc_state[0])\n",
    "\n",
    "    with tf.compat.v1.variable_scope(\"decode\"):\n",
    "        training_decoder = training_decoding_layer(dec_embed_input, \n",
    "                                                  summary_length, \n",
    "                                                  dec_cell, \n",
    "                                                  initial_state,\n",
    "                                                  output_layer,\n",
    "                                                  vocab_size, \n",
    "                                                  max_summary_length)\n",
    "        \n",
    "        training_logits, _, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                  output_time_major=False,\n",
    "                                  impute_finished=True,\n",
    "                                  maximum_iterations=max_summary_length)\n",
    "\n",
    "    with tf.compat.v1.variable_scope(\"decode\", reuse=True):\n",
    "        inference_decoder = inference_decoding_layer(embeddings,  \n",
    "                                                    vocab_to_int[''], \n",
    "                                                    vocab_to_int[''],\n",
    "                                                    dec_cell, \n",
    "                                                    initial_state, \n",
    "                                                    output_layer,\n",
    "                                                    max_summary_length,\n",
    "                                                    batch_size)\n",
    "        \n",
    "        inference_logits, _, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                  output_time_major=False,\n",
    "                                  impute_finished=True,\n",
    "                                  maximum_iterations=max_summary_length)\n",
    "\n",
    "    return training_logits, inference_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
    "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
    "    embeddings = word_embedding_matrix\n",
    "    \n",
    "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
    "    \n",
    "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
    "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "    \n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
    "                                                        embeddings,\n",
    "                                                        enc_output,\n",
    "                                                        enc_state, \n",
    "                                                        vocab_size, \n",
    "                                                        text_length, \n",
    "                                                        summary_length, \n",
    "                                                        max_summary_length,\n",
    "                                                        rnn_size, \n",
    "                                                        vocab_to_int, \n",
    "                                                        keep_prob, \n",
    "                                                        batch_size,\n",
    "                                                        num_layers)\n",
    "    \n",
    "    return training_logits, inference_logits\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch):\n",
    "    \"\"\"Pad sentences with  so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(summaries, texts, batch_size):\n",
    "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(texts)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        texts_batch = texts[start_i:start_i + batch_size]\n",
    "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
    "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "        \n",
    "        pad_texts_lengths = []\n",
    "        for text in pad_texts_batch:\n",
    "            pad_texts_lengths.append(len(text))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "epochs = 10 # 100\n",
    "batch_size = 4\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-07 15:05:46,802 - tensorflow - WARNING\n",
      "Msg: From C:\\Users\\Burak\\AppData\\Local\\Temp\\ipykernel_8664\\611900757.py:4: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Burak\\AppData\\Local\\Temp\\ipykernel_8664\\265531533.py\", line 5, in build_graph  *\n        training_logits, inference_logits = seq2seq_model(\n    File \"C:\\Users\\Burak\\AppData\\Local\\Temp\\ipykernel_8664\\1271019391.py\", line 9, in seq2seq_model  *\n        enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n    File \"C:\\Users\\Burak\\AppData\\Local\\Temp\\ipykernel_8664\\1646290235.py\", line 9, in encoding_layer  *\n        cell_fw = tf.keras.layers.LSTM(rnn_size,\n    File \"c:\\Users\\Burak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\lstm.py\", line 463, in __init__  **\n        cell = LSTMCell(\n    File \"c:\\Users\\Burak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\lstm.py\", line 133, in __init__\n        self.dropout = min(1.0, max(0.0, dropout))\n\n    OperatorNotAllowedInGraphError: Using a symbolic `tf.Tensor` as a Python `bool` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m     input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length \u001b[38;5;241m=\u001b[39m model_inputs()\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# Execute graph build\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m     cost, inference_logits \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummary_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_summary_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraph is built.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Burak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Burak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\autograph_util.py:52\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Burak\\AppData\\Local\\Temp\\ipykernel_8664\\265531533.py\", line 5, in build_graph  *\n        training_logits, inference_logits = seq2seq_model(\n    File \"C:\\Users\\Burak\\AppData\\Local\\Temp\\ipykernel_8664\\1271019391.py\", line 9, in seq2seq_model  *\n        enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n    File \"C:\\Users\\Burak\\AppData\\Local\\Temp\\ipykernel_8664\\1646290235.py\", line 9, in encoding_layer  *\n        cell_fw = tf.keras.layers.LSTM(rnn_size,\n    File \"c:\\Users\\Burak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\lstm.py\", line 463, in __init__  **\n        cell = LSTMCell(\n    File \"c:\\Users\\Burak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\lstm.py\", line 133, in __init__\n        self.dropout = min(1.0, max(0.0, dropout))\n\n    OperatorNotAllowedInGraphError: Using a symbolic `tf.Tensor` as a Python `bool` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\n"
     ]
    }
   ],
   "source": [
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      text_length,\n",
    "                                                      summary_length,\n",
    "                                                      max_summary_length,\n",
    "                                                      len(vocab_to_int)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers, \n",
    "                                                      vocab_to_int,\n",
    "                                                      batch_size)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(targets, training_logits)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.get_gradients(cost, training_logits)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in zip(gradients, train_graph.variables) if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "\n",
    "print(\"Graph is built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the data for training\n",
    "start = 2000\n",
    "end = start + 3000\n",
    "sorted_summaries_short = sorted_summaries[start:end]\n",
    "sorted_texts_short = sorted_texts[start:end]\n",
    "print(\"The shortest text length:\", len(sorted_texts_short[0]))\n",
    "print(\"The longest text length:\",len(sorted_texts_short[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 20 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 6 #3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "  \n",
    "tf.reset_default_graph()\n",
    "checkpoint = \"best_model.ckpt\"  #300k sentence\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # If we want to continue training a previous session\n",
    "    # loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    # loader.restore(sess, checkpoint)\n",
    "    #sess.run(tf.local_variables_initializer())\n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: texts_batch,\n",
    "                 targets: summaries_batch,\n",
    "                 lr: learning_rate,\n",
    "                 summary_length: summaries_lengths,\n",
    "                 text_length: texts_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(sorted_texts_short) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "                \n",
    "                #saver = tf.train.Saver() \n",
    "                #saver.save(sess, checkpoint)\n",
    "                \n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "              \n",
    "                  \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('New Record!') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "            \n",
    "                    \n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learning_rate *= learning_rate_decay\n",
    "        if learning_rate < min_learning_rate:\n",
    "            learning_rate = min_learning_rate\n",
    "        \n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training.\")\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
