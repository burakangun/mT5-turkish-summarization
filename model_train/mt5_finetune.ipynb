{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ekY1xSZaPV6x"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from datasets import load_dataset\n",
        "from transformers import TFMT5ForConditionalGeneration, MT5Tokenizer, DataCollatorForSeq2Seq\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Sq8j-TtSJNL",
        "outputId": "425733f2-f5bd-431d-d604-f06ec37b1827"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "AnCFjnROPV6z"
      },
      "outputs": [],
      "source": [
        "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5pNjX-tPV61",
        "outputId": "3ba4cd3a-d2a7-4184-e9cf-88284155585b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['abstract', 'author', 'content', 'date', 'source', 'tags', 'title', 'topic', 'url'],\n",
            "        num_rows: 138786\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['abstract', 'author', 'content', 'date', 'source', 'tags', 'title', 'topic', 'url'],\n",
            "        num_rows: 14610\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['abstract', 'author', 'content', 'date', 'source', 'tags', 'title', 'topic', 'url'],\n",
            "        num_rows: 15379\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "import pandas as pd\n",
        "\n",
        "# Hugging Face'ten veri setini yÃ¼kleme\n",
        "dataset = load_dataset(\"batubayk/TR-News\")  # Kendi veri setinizin adÄ±nÄ± yazÄ±n\n",
        "\n",
        "# Train ve validation setlerini alÄ±n\n",
        "train_dataset = dataset['train']\n",
        "val_dataset = dataset['validation']\n",
        "test_dataset = dataset['test']\n",
        "# Train dataset'in yarÄ±sÄ±nÄ± alma\n",
        "train_half = train_dataset.select(range(len(train_dataset) // 2))\n",
        "\n",
        "\n",
        "# TÃ¼m veri setlerini bir DatasetDict'e dÃ¶nÃ¼ÅŸtÃ¼rme\n",
        "combined_datasets = DatasetDict({\n",
        "    'train': train_half,       # YarÄ±ya indirgenmiÅŸ train set\n",
        "    'validation': val_dataset, # Validation set\n",
        "    'test': test_dataset       # Test set\n",
        "})\n",
        "\n",
        "# BirleÅŸtirilmiÅŸ veri setini gÃ¶rÃ¼ntÃ¼leme\n",
        "print(combined_datasets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ympBvHoxPV64",
        "outputId": "d1aa1e0f-f0aa-49ba-ece2-7f129b03432b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 138786\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 14610\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 15379\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "def tokenize_sample_data(data):\n",
        "\n",
        "    input_feature = tokenizer(data['content'], truncation=True, max_length=1024)\n",
        "    label = tokenizer(data['abstract'], truncation=True, max_length=128)\n",
        "    return {\n",
        "        \"input_ids\" : input_feature['input_ids'],\n",
        "        \"attention_mask\" : input_feature['attention_mask'],\n",
        "        \"labels\" : label['input_ids'],\n",
        "    }\n",
        "\n",
        "tokenized_ds = combined_datasets.map(\n",
        "    tokenize_sample_data,\n",
        "    remove_columns= ['abstract','author','content','date','source','tags','title','topic','url'],\n",
        "    batched=True,\n",
        "    batch_size= 512\n",
        ")\n",
        "tokenized_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "hQwKEJ42PV65",
        "outputId": "f7e31f11-9f49-424c-ff93-80d05e663082"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nfrom transformers import MT5Tokenizer, MT5ForConditionalGeneration\\n\\n# 1. Tokenizer ve modelin yÃ¼klenmesi\\nmodel_name = \"google/mt5-small\"\\ntokenizer = MT5Tokenizer.from_pretrained(model_name)\\nmodel = MT5ForConditionalGeneration.from_pretrained(model_name)\\n\\n# 2. Ã–zetlenmek istenen TÃ¼rkÃ§e metin\\nturkish_text = \"\"\"\\nÄ°klim deÄŸiÅŸikliÄŸi, kÃ¼resel sÄ±caklÄ±klarÄ±n artÄ±ÅŸÄ±yla birlikte ortaya Ã§Ä±kan Ã§evresel, sosyal ve ekonomik sorunlara neden olmaktadÄ±r.\\nÃ–zellikle kuraklÄ±k, sel ve orman yangÄ±nlarÄ± gibi doÄŸal afetlerin sÄ±klÄ±ÄŸÄ± artarken, tarÄ±msal Ã¼retimde de ciddi dÃ¼ÅŸÃ¼ÅŸler yaÅŸanmaktadÄ±r.\\nBu durum, gÄ±da gÃ¼venliÄŸini tehdit etmekte ve toplumlarÄ± olumsuz yÃ¶nde etkilemektedir.\\n\"\"\"\\n\\n# 3. Ã–zetleme iÃ§in giriÅŸ formatÄ±: \\'summarize:\\' Ã¶n ekini ekliyoruz\\ninput_text = f\"summarize: {turkish_text}\"\\n\\n# 4. Tokenizer ile metni encode etme\\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\\n\\n# 5. Modeli kullanarak Ã¶zetleme yapma\\nsummary_ids = model.generate(\\n    input_ids,\\n    max_length=150,\\n    min_length=30,\\n    length_penalty=2.0,\\n    num_beams=4,\\n    early_stopping=True\\n)\\n\\n# 6. Ã–zetlenen metni decode etme\\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\\n\\n# 7. Sonucu yazdÄ±rma\\nprint(\"Ã–zetlenen Metin:\")\\nprint(summary)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "'''\n",
        "\n",
        "from transformers import MT5Tokenizer, MT5ForConditionalGeneration\n",
        "\n",
        "# 1. Tokenizer ve modelin yÃ¼klenmesi\n",
        "model_name = \"google/mt5-small\"\n",
        "tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
        "model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# 2. Ã–zetlenmek istenen TÃ¼rkÃ§e metin\n",
        "turkish_text = \"\"\"\n",
        "Ä°klim deÄŸiÅŸikliÄŸi, kÃ¼resel sÄ±caklÄ±klarÄ±n artÄ±ÅŸÄ±yla birlikte ortaya Ã§Ä±kan Ã§evresel, sosyal ve ekonomik sorunlara neden olmaktadÄ±r.\n",
        "Ã–zellikle kuraklÄ±k, sel ve orman yangÄ±nlarÄ± gibi doÄŸal afetlerin sÄ±klÄ±ÄŸÄ± artarken, tarÄ±msal Ã¼retimde de ciddi dÃ¼ÅŸÃ¼ÅŸler yaÅŸanmaktadÄ±r.\n",
        "Bu durum, gÄ±da gÃ¼venliÄŸini tehdit etmekte ve toplumlarÄ± olumsuz yÃ¶nde etkilemektedir.\n",
        "\"\"\"\n",
        "\n",
        "# 3. Ã–zetleme iÃ§in giriÅŸ formatÄ±: 'summarize:' Ã¶n ekini ekliyoruz\n",
        "input_text = f\"summarize: {turkish_text}\"\n",
        "\n",
        "# 4. Tokenizer ile metni encode etme\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "# 5. Modeli kullanarak Ã¶zetleme yapma\n",
        "summary_ids = model.generate(\n",
        "    input_ids,\n",
        "    max_length=150,\n",
        "    min_length=30,\n",
        "    length_penalty=2.0,\n",
        "    num_beams=4,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "# 6. Ã–zetlenen metni decode etme\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# 7. Sonucu yazdÄ±rma\n",
        "print(\"Ã–zetlenen Metin:\")\n",
        "print(summary)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Kvsu1jtiPV66"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoConfig, AutoModelForSeq2SeqLM\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Model konfigÃ¼rasyonu\n",
        "mt5_config = AutoConfig.from_pretrained(\n",
        "    \"google/mt5-small\",\n",
        "    max_length=256,\n",
        "    length_penalty=1.0,\n",
        "    no_repeat_ngram_size=3,\n",
        "    num_beams=5,\n",
        ")\n",
        "\n",
        "# PyTorch tabanlÄ± modeli yÃ¼kleme ve GPU/CPU'ya taÅŸÄ±ma\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-small\", config=mt5_config).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "kL7j5kNqPV67"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(\n",
        "  tokenizer,\n",
        "  model=model,\n",
        "  return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "o9p6BzTgPV67"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "\n",
        "# define function for custom tokenization\n",
        "def tokenize_sentence(arg):\n",
        "  encoded_arg = tokenizer(arg)\n",
        "  return tokenizer.convert_ids_to_tokens(encoded_arg.input_ids)\n",
        "\n",
        "# define function to get ROUGE scores with custom tokenization\n",
        "def metrics_func(eval_arg):\n",
        "  preds, labels = eval_arg\n",
        "  # Replace -100\n",
        "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "  # Convert id tokens to text\n",
        "  text_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "  text_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "  # Insert a line break (\\n) in each sentence for ROUGE scoring\n",
        "  # (Note : Please change this code, when you perform on other languages except for Japanese)\n",
        "  text_preds = [(p if p.endswith((\"!\", \"ï¼\", \"?\", \"ï¼Ÿ\", \"ã€‚\")) else p + \"ã€‚\") for p in text_preds]\n",
        "  text_labels = [(l if l.endswith((\"!\", \"ï¼\", \"?\", \"ï¼Ÿ\", \"ã€‚\")) else l + \"ã€‚\") for l in text_labels]\n",
        "  sent_tokenizer_jp = RegexpTokenizer(u'[^!ï¼?ï¼Ÿã€‚]*[!ï¼?ï¼Ÿã€‚]')\n",
        "  text_preds = [\"\\n\".join(np.char.strip(sent_tokenizer_jp.tokenize(p))) for p in text_preds]\n",
        "  text_labels = [\"\\n\".join(np.char.strip(sent_tokenizer_jp.tokenize(l))) for l in text_labels]\n",
        "  # compute ROUGE score with custom tokenization\n",
        "  return rouge_metric.compute(\n",
        "    predictions=text_preds,\n",
        "    references=text_labels,\n",
        "    tokenizer=tokenize_sentence\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "!pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ED_Q-uAc16eV",
        "outputId": "555bfafb-0e1b-4bb5-8f8a-a64687ae8b16"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.9)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "508khiopPV67",
        "outputId": "049176a2-3c41-49b8-e1a4-eafaa4473c7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"mt5-summarize-ja\",\n",
        "    log_level=\"error\",\n",
        "    num_train_epochs=10,\n",
        "    learning_rate=2e-5,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    warmup_steps=500,\n",
        "    optim=\"adafactor\",\n",
        "    weight_decay=0.01,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=256,\n",
        "    save_steps=500,\n",
        "    logging_steps=50,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "GdDg8SNEPV68",
        "outputId": "67c385c4-21de-4b2f-e726-bedcdf74b47a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-11a26fc2bb51>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2122\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2123\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2124\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2125\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2479\u001b[0m                     )\n\u001b[1;32m   2480\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2481\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2483\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3610\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3611\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3612\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3613\u001b[0m             \u001b[0;31m# Finally we need to normalize the loss for reporting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3614\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2240\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2241\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=metrics_func,\n",
        "    train_dataset=tokenized_ds[\"train\"],\n",
        "    eval_dataset=tokenized_ds[\"validation\"].select(range(20)),\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''# EÄŸitilen modeli kaydet\n",
        "output_dir = \"./trained_model\"  # Kaydedilecek dizin\n",
        "trainer.save_model(output_dir)  # Modeli ve tokenizer'Ä± kaydeder\n",
        "\n",
        "# Tokenizer'Ä± ayrÄ±ca kaydetmek isterseniz\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"Model ve tokenizer {output_dir} dizinine kaydedildi.\")'''"
      ],
      "metadata": {
        "id": "1yY9QcafhjpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# EÄŸitilen modeli ve tokenizer'Ä± yÃ¼kle\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"./trained_model\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./trained_model\")\n",
        "'''"
      ],
      "metadata": {
        "id": "piGBJOV-ibHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''test_text = \"DÃ¼nyamÄ±zda 1900â€™lÃ¼ yÄ±llardan gÃ¼nÃ¼mÃ¼ze kadar dil Ã¶ÄŸretiminde Ã§eÅŸitli metinler kullanÄ±lmÄ±ÅŸtÄ±r. Bunlar â€œ edebi metinler, Ã¼retilmiÅŸ metinler, Ã¶zgÃ¼n ve Ã¶zel metinler â€ baÅŸlÄ±klarÄ± altÄ±nda toplanmÄ±ÅŸtÄ±r. Metinlerin seÃ§imi dil Ã¶ÄŸretim yaklaÅŸÄ±m ve yÃ¶ntemlerine gÃ¶re deÄŸiÅŸmektedir. Her yaklaÅŸÄ±m kendine Ã¶zgÃ¼ metin kullanmÄ±ÅŸtÄ±r. Geleneksel yaklaÅŸÄ±mda dil bilgisi kurallarÄ±, atasÃ¶zleri, edebiyat, genel kÃ¼ltÃ¼r gibi konularÄ±n Ã¶ÄŸretimine aÄŸÄ±rlÄ±k verildiÄŸinden edebi metinler kullanÄ±lmÄ±ÅŸtÄ±r. DavranÄ±ÅŸÃ§Ä± yaklaÅŸÄ±mda dil davranÄ±ÅŸ olarak ele alÄ±nmÄ±ÅŸ, tekrar, taklit ve ezberleme yoluyla Ã¶ÄŸretilmiÅŸtir. Bu yaklaÅŸÄ±mda edebi metinler yerine Ã¼retilmiÅŸ metinler kullanÄ±lmÄ±ÅŸtÄ±r. BiliÅŸsel yaklaÅŸÄ±mda â€œdil iletiÅŸim aracÄ±dÄ±râ€ gÃ¶rÃ¼ÅŸÃ¼ yayÄ±lmÄ±ÅŸ ve Ã¶zgÃ¼n metinler kullanÄ±lmaya baÅŸlanmÄ±ÅŸtÄ±r. \"\n",
        "'''"
      ],
      "metadata": {
        "id": "VKEEh23hislk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''inputs = tokenizer(\n",
        "    test_text,\n",
        "    return_tensors=\"pt\",  # PyTorch tensÃ¶rleri olarak dÃ¶ner\n",
        "    max_length=1024,      # Modelin giriÅŸ sÄ±nÄ±rÄ±\n",
        "    truncation=True       # Ã‡ok uzun metinleri keser\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "id": "P0vpkXDdimu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Ã–zetleme\n",
        "output_ids = model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    attention_mask=inputs[\"attention_mask\"],\n",
        "    max_length=128,  # Ã–zet uzunluÄŸu sÄ±nÄ±rÄ±\n",
        "    num_beams=4,     # Beam search kullanÄ±mÄ±\n",
        "    no_repeat_ngram_size=2,  # Tekrar eden n-gramlarÄ± engeller\n",
        "    length_penalty=0.6       # KÄ±sa Ã¶zetlere Ã¶ncelik\n",
        ")\n",
        "\n",
        "# Ã–zet Ã§Ã¶zÃ¼mleme\n",
        "summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(\"Ã–zet:\", summary)\n",
        "'''"
      ],
      "metadata": {
        "id": "RNB0YFddixHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''from google.colab import drive\n",
        "drive.mount('/content/drive')'''"
      ],
      "metadata": {
        "id": "aSDoLGHwj3E9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}